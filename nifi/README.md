# NiFi Deployment Instruction

## 1. AWS Client VPN

### Pre-Requisites

1. VPN Client Software
   1. [AWS-provided client](https://docs.aws.amazon.com/vpn/latest/clientvpn-user/connect-aws-client-vpn-connect.html) or
   2. [OpenVPN client](https://docs.aws.amazon.com/vpn/latest/clientvpn-user/connect.html)
2. CloudFormation template: `client_vpn_stack.yaml`
3. AWS CLI configured with appropriate credentials

### Deployment Steps

1. Create a stack using the `client_vpn_stack.yaml` template

   1. Parameters:

      1. General Configuration

         1. Resource Suffix: Unique identifier to prevent resource naming conflicts, to allow multiple stacks to be created.

      2. Root CA Configuration

         1. Root Key:
            1. Copy and paste the output after running the following commands in a secure environment: `openssl genrsa -out ca.key 2048 && cat ca.key`
         2. Root Certificate:
            1. Copy and paste the output after running the following commands in a secure environment: `openssl req -x509 -new -key ca.key -sha256 -days 3650 -out ca.pem -subj /CN=RootCA/OU=client-vpn/O=DoD/C=US && cat ca.pem`. You can substitute different values for `days` or `subj` flags.
         3. (**_IMPORTANT_**) After copying the values, securely delete the files: `shred -u ca.key ca.pem`

      3. Client VPN Endpoint Configuration

         1. Server Certificate ARN: Leave this **blank** when creating the stack. After the successful creation, you'll update the stack, by inputting a value generated by running commands in the Outputs section.
         2. Client CIDR Block:
            1. The IPv4 address range in CIDR notation, from which to assign client IP addresses.
            2. (**_IMPORTANT_**) This parameter **CANNOT** be updated after the initial stack creation. The address range **MUST NOT** overlap with the local CIDR of the VPC in which the associated subnet is located, or the routes that you add manually. The address range must have a size of at least /22 and not be greater than /12; _if unsure, specify /16 or larger_.
         3. VPC ID: Choose the VPC where NiFi cluster(s) will be deployed.
         4. VPC CIDR: Self-explanatory
         5. Private Subnet 1: Self-explanatory
         6. Private Subnet 2: Self-explanatory. Choose one in a **DIFFERENT** availability zone than Private Subnet 1.
         7. Session Timeout Hours: Self-explanatory
         8. DNS Server Addresses: (_Optional_) A comma-separated list of up to two custom DNS servers. If you want to resolve private resources within the VPC, for example, use the IP address of the base of the VPC network range plus two. If no DNS server is specified, the DNS address configured on the device is used for the DNS server.
         9. Banner Text: (_Optional_) Customizable text that will be displayed in a banner on AWS-provided clients when a VPN session is established. UTF-8 encoded characters only. Maximum of 1400 characters. If not provided, no banner will be displayed.

      4. Authorized Client Common Names
         1. Authorized Common Names: Enter `ALL` to authorize all clients; `NONE` to deny all clients; or a list of comma-separated common names of clients from their certificates to authorize. This acts as an additional layer of access control.

   2. Once the stack has been created, enable termination protection to prevent accidental deletion.

2. Update the stack

   1. Navigate to the `Outputs` section for the stack.
   2. Copy the long series of commands in the `HowToCreateServerKeyAndCert` output value and run them in a secure environment with your AWS credentials. When prompted, enter the Common Name of your choosing.
   3. If the command ran successfully, you'll get an output like below:

   ```
   {
      "CertificateArn": "arn:aws-us-gov:acm:us-gov-west-1:123456789012:certificate/1f23bb9e-abcd-abcd-efgh-c233b786ddc6"
   }
   ```

   4. Copy the `CertificateArn` value and update the stack by pasting it in Server Certificate ARN parameter.

3. Generate VPN configuration files for users

   1. Once the stack has been updated successfully, navigate to the `Outputs` section.
   2. Copy the long series of commands in the `HowToCreateClientKeyAndCert` output value and run them in a secure environment with your AWS credentials. When prompted, enter the Common Name for a user you are creating the file for.
   3. If the command ran successfully, you'll get an output like below:

   ```
   Send to the intended user the config file at the following location: /tmp/tmp.iPjnkjguUn/John.Lennon_config_file.ovpn

   ***IMPORTANT*** Delete the config file using the following command:
   shred -u /tmp/tmp.iPjnkjguUn/John.Lennon_config_file.ovpn
   ```

   4. Using the above example output, securely transfer `/tmp/tmp.iPjnkjguUn/John.Lennon_config_file.ovpn` file (which contains a private key and cert for the client) to the intended user and run `shred -u /tmp/tmp.iPjnkjguUn/John.Lennon_config_file.ovpn` to delete the file.
   5. Repeat the above steps for each user requiring access.

4. Connect using Client VPN Endpoint

   1. Users who wish to connect need to install a VPN Client Software.
   2. For AWS-provided VPN Client Software:
      1. Create a profile by going to `File` and selecting `Manage Profiles`.
      2. Set a Display Name and select the configuration file generated for them by you.
      3. Connect using the created profile.

5. Clean-up
   1. Delete the stack.
   2. Remove server certificates from AWS Certificate Manager (ACM).

### Troubleshooting

1. If connection issues occur, ensure no conflicting VPN connections are active.
2. Verify correct DNS settings if unable to resolve internal resources.
3. Check AWS VPC and security group configurations if experiencing connectivity problems to specific resources.

### Future Enhancements:

1. Implement AWS Private Certificate Authority for improved certificate management

### Notes

1. How to convert CRL from ACM to PEM format compatible with Client VPN
   `openssl crl -inform DER -in acm_crl.crl -outform PEM -out acm_crl.pem`

## 2. ZooKeeper Cluster

### Pre-Requisites

1. CloudFormation template: `aftac_zk_stack.yaml`
2. VPC with the following VPC endpoints:
   1. s3 (gateway)
   2. ssm (interface)
   3. ssmmessages (interface)
   4. ec2messages (interface)

### Deployment Steps

1. Create a stack using the `aftac_zk_stack.yaml` template

   1. Parameters:

      1. General Configuration

         1. IAM Resource Prefix: Read-Only
         2. Permissions Boundary Policy ARN: Self-explanatory
         3. Resource Suffix: Unique identifier to prevent resource naming conflicts, to allow multiple stacks to be created.

      2. ZooKeeper Configuration

         1. ZooKeeper Instance Type: Self-explanatory
         2. ZooKeeper Cluster Size: Self-explanatory
         3. ZooKeeper Binary S3 URI: Self-explanatory

      3. Networking Configuration

         1. VPC ID: Self-explanatory
         2. VPC CIDR: Self-explanatory
         3. Subnet IDs: Self-explanatory
         4. S3 Prefix List ID: Self-explanatory

      4. Image Builder Configuration

         1. Semantic Version: Increment this when updating the parent image below or ImageBuilder-related resources in the template.
            The format is <major>.<minor>.<patch> where each component is an integer (e.g. 1.0.0).
         2. Parent Image: Base image for the image recipe
         3. Proxy Server Address: Self-explanatory (exclude the port number).
         4. No Proxy List: Self-explanatory
         5. Bypass GPG Check: Self-explanatory
         6. Outbound CIDR: Self-explanatory

   2. Once the stack has been created, enable termination protection to prevent accidental deletion.

2. Set Auto Scaling Group to use the default version of LaunchTemplate

   1. Navigate to the `Resources` section for the stack.
   2. Click the link next to `ZkAutoScalingGroup` logical ID.
   3. Click `Edit` button.
   4. Under `Launch template` section, click the dropdown under `Version` field and select `Default (1)` .
   5. Click `Update` button at the bottom of the page.

3. Create a ZooKeeper image

   1. Navigate back to the `Resources` section for the stack.
   2. Click the link next to `ZkImageBuilderPipeline` logical ID.
   3. Click `Actions` button and select `Run pipeline`.
   4. Monitor the progress and wait till `Image status` in `Output images` section changes to `Available` state. This could take around 20 minutes or so.

4. Refresh instances in Auto Scaling Group

   1. Navigate back to the `Resources` section for the stack.
   2. Click the link next to `ZkAutoScalingGroup` logical ID.
   3. Click `Instance refresh` tab in the bottom pane.
   4. Click `Start instance refresh` button.
   5. Set healthy percentage min to `0` and click `Start instance fresh` button at the bottom.

5. Verify ZooKeeper cluster is functioning

   1. After the instance refresh is complete, log into each instance in the ZooKeeper cluster using SSM Session Manager.
   2. Run the following commands:

   ```
   sudo su -
   cd $ZK_HOME/bin
   ./zkServer.sh status
   ```

   3. The output should look like this:

   ```
   /usr/bin/java
   ZooKeeper JMX enabled by default
   Using config: /root/apache-zookeeper-3.8.4-bin/bin/../conf/zoo.cfg
   Client port not found in static config file. Looking in dynamic config file.
   Client port found: 2181. Client address: x.x.x.x. Client SSL: false.
   Mode: leader|follower
   ```

   4. If not, run `./zkServer.sh start`, wait a few seconds, and run `./zkServer.sh status` to check the status again.

### Troubleshooting

1. If there are any issues with the NiFi, check the logs located at `$ZK_HOME/logs`.
2. If you make changes to the LaunchTemplate resource in the template after the deployment, follow the steps 2 and 3 above to set the Auto Scaling Group to use the default version of the LaunchTemplate and create another image.

### Future Enhancements:

1. Enable secure client-server communication.

## 3. NiFi Cluster

### Pre-Requisites

1. CloudFormation template: `aftac_nifi_stack.yaml`
2. VPC with the following VPC endpoints:
   1. s3 (gateway)
   2. ssm (interface)
   3. ssmmessages (interface)
   4. ec2messages (interface)
   5. secretsmanager (interface)

### Deployment Steps

1. Create a stack using the `aftac_nifi_stack.yaml` template

   1. Parameters:

      1. General Configuration

         1. IAM Resource Prefix: Read-Only
         2. Permissions Boundary Policy ARN: Self-explanatory
         3. Resource Suffix: Unique identifier to prevent resource naming conflicts, to allow multiple stacks to be created.

      2. Root CA Configuration

         1. Root Key:
            1. Copy and paste the output after running the following commands in a secure environment: `openssl genrsa -out ca.key 2048 && cat ca.key`
         2. Root Certificate:
            1. Copy and paste the output after running the following commands in a secure environment: `openssl req -x509 -new -key ca.key -sha256 -days 3650 -out ca.pem -subj /CN=RootCA/OU=nifi/O=DoD/C=US && cat ca.pem`. You can substitute different values for `days` or `subj` flags.
         3. (**_IMPORTANT_**) After copying the values, securely delete the files: `shred -u ca.key ca.pem`

      3. NiFi Configuration

         1. NiFi Admin User: Self-explanatory
         2. NiFi Instance Type: Self-explanatory
         3. NiFi Cluster Size: Self-explanatory
         4. NiFi Binary S3 URI: Self-explanatory
         5. DoD Cert Bundle S3 URI: Self-explanatory
         6. ZooKeeper Cluster Information Parameter: Paste in `ZkClusterInfoParameterName` value from the `Outputs` section of the ZooKeeper cluster stack.

      4. Networking Configuration

         1. VPC ID: Self-explanatory
         2. VPC CIDR: Self-explanatory
         3. Subnet IDs: Self-explanatory
         4. S3 Prefix List ID: Self-explanatory

      5. Image Builder Configuration

         1. Semantic Version: Increment this when updating the parent image below or ImageBuilder-related resources in the template.
            The format is <major>.<minor>.<patch> where each component is an integer (e.g. 1.0.0).
         2. Parent Image: Base image for the image recipe
         3. Proxy Server Address: Self-explanatory (exclude the port number).
         4. No Proxy List: Self-explanatory
         5. Bypass GPG Check: Self-explanatory
         6. Outbound CIDR: Self-explanatory

   2. Once the stack has been created, enable termination protection to prevent accidental deletion.

2. Set Auto Scaling Group to use the default version of LaunchTemplate

   1. Navigate to the `Resources` section for the stack.
   2. Click the link next to `NifiAutoScalingGroup` logical ID.
   3. Click `Edit` button.
   4. Under `Launch template` section, click the dropdown under `Version` field and select `Default (1)` .
   5. Click `Update` button at the bottom of the page.

3. Create a NiFi image

   1. Navigate back to the `Resources` section for the stack.
   2. Click the link next to `NifiImageBuilderPipeline` logical ID.
   3. Click `Actions` button and select `Run pipeline`.
   4. Monitor the progress and wait till `Image status` in `Output images` section changes to `Available` state. This could take around 20 minutes or so.

4. Refresh instances in Auto Scaling Group

   1. Navigate back to the `Resources` section for the stack.
   2. Click the link next to `NifiAutoScalingGroup` logical ID.
   3. Click `Instance refresh` tab in the bottom pane.
   4. Click `Start instance refresh` button.
   5. Set healthy percentage min to `0` and click `Start instance fresh` button at the bottom.

5. Verify NiFi cluster is functioning

   1. After the instance refresh is complete, log into each instance in the NiFi cluster using SSM Session Manager.
   2. Run the following commands:

   ```
   sudo su -
   cd $NIFI_HOME/bin
   ./nifi.sh status
   ```

   3. The output should look like this:

   ```
   INFO [main] org.apache.nifi.bootstrap.Command Application Process [6819] Command Status [SUCCESS] HTTP 200
   INFO [main] org.apache.nifi.bootstrap.Command Status: UP
   ```

   4. If, instead, the output looks like below, then run `./nifi.sh start`. Wait a couple of minutes before running `./nifi.sh status` to check the status again.

   ```
   org.apache.nifi.bootstrap.Command Application Process [6819] Management Server [http://127.0.0.1:52020/health] communication failed
   ```

### Troubleshooting

1. If there are any issues with the NiFi, check the logs located at `$NIFI_HOME/logs`.
2. If you make changes to the LaunchTemplate resource in the template after the deployment, follow the steps 2 and 3 above to set the Auto Scaling Group to use the default version of the LaunchTemplate and create another image.
3. If you add additional nodes to an existing cluster, it requires manual process to synchronize authorizations.xml, users.xml, and flow.json.gz across all nodes, both existing and new.

### Future Enhancements:

1. Create a NiFi user and specify it bootstrap.conf
2. Run NiFi as a service
3. Customize repos.
4. Add NiFi Registry
5. Specify heap size in bootstrap.conf?

## 4. Access NiFi cluster

1. Ensure you're connected to the VPN using Client VPN endpoint.
2. Retrieve the private IP address of the instance you want to log into.
3. Type the following in the address bar of your browser, replacing the `IP_Address` with the actual IP address: `https://IP_Address:8443/nifi`
4. If you are the admin, perform admin tasks such as adding additional users and creating/updating policies.

# TODOs:

1. ZooKeeper:
   1. Reference: https://zookeeper.apache.org/doc/current/zookeeperAdmin.html
   2. Having a supervisory process such as daemontools or SMF (other options for supervisory process are also available, it's up to you which one you would like to use, these are just two examples) managing your ZooKeeper server ensures that if the process does exit abnormally it will automatically be restarted and will quickly rejoin the cluster.
      1. Set it up in a way that sends a notification to SNS topic for monitoring?
   3. Use secureClientPort
   4. Be careful where you put the transaction log. A dedicated transaction log device is key to consistent good performance. Putting the log on a busy device will adversely affect performance.
      1. dataLogDir : This option will direct the machine to write the transaction log to the dataLogDir rather than the dataDir. This allows a dedicated log device to be used, and helps avoid competition between logging and snapshots. Having a dedicated log device has a large impact on throughput and stable latencies. It is highly recommended dedicating a log device and set dataLogDir to point to a directory on that device, and then make sure to point dataDir to a directory not residing on that device. The most performance critical part of ZooKeeper is the transaction log. ZooKeeper syncs transactions to media before it returns a response. A dedicated transaction log device is key to consistent good performance. Putting the log on a busy device will adversely affect performance. If you only have one storage device, increase the snapCount so that snapshot files are generated less often; it does not eliminate the problem, but it makes more resources available for the transaction log.
